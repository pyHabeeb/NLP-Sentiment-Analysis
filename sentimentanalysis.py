# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PdSfuVGf339O2VX4KXDfBvLwJIeFXSwA
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("jp797498e/twitter-entity-sentiment-analysis")

print("Path to dataset files:", path)

import pandas as pd
import os
import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet#,stopwords
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
from tqdm import tqdm
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')
tqdm.pandas()
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

#path = "/kaggle/input/twitter-entity-sentiment-analysis"

csv_file = os.path.join(path, 'twitter_training.csv')
column_names = ['tweet_id', 'entity','sentiment','tweet_text']
df = pd.read_csv(csv_file, header=None, names = column_names)

from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))


#To drop null values from the dataframe
df.dropna(subset=['tweet_text'], inplace = True)


print("\n---Starting Text Processing---")
def clean_text(text):
  text = re.sub(r'http\S+|www\S+|https\S+', '',text, flags = re.MULTILINE)
  text = re.sub(r'@\w+', '', text)
  text = re.sub (r'[^A-Za-z\s]', '',text)
  return text.strip().lower()
def lemmatize_text(text):
  lemmatizer = WordNetLemmatizer()
  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))
  wordnet_tagged = map(lambda x: (x[0],nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)
  lemmatized_sentence = [lemmatizer.lemmatize(word, tag) if tag else word for word, tag in wordnet_tagged]
  return ' '.join(lemmatized_sentence)
def nltk_tag_to_wordnet_tag(nltk_tag):
  if nltk_tag.startswith('J'):
    return wordnet.ADJ
  elif nltk_tag.startswith('V'):
    return wordnet.VERB
  elif nltk_tag.startswith('N'):
    return wordnet.NOUN
  elif nltk_tag.startswith('R'):
    return wordnet.ADV
  else:
    return None
#Function to remove words like is, the etc from the tweets
def remove_stopwords(text):
  tokens = text.split()
  return ' '.join([word for word in tokens if word not in stop_words])
df['cleaned_text'] = df['tweet_text'].progress_apply(clean_text)
df['processed_text'] = df['cleaned_text'].progress_apply(lemmatize_text)
df['processed_text'] = df['processed_text'].progress_apply(remove_stopwords)
print("\n---Preprocessing Completed---")

X = df['processed_text']
y = df['sentiment']
X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f'Training Set Size: {len(X_train)}')
print(f'Testing Set Size: {len(X_test)}')

vectorizer = TfidfVectorizer(ngram_range = (1, 2), max_features = 10000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)


print('\n---Training Model---')
model = MultinomialNB()
model.fit(X_train_vec, y_train)
print('\n---Model Training Completed---')

y_pred = model.predict(X_test_vec)

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("\n--- Model Evaluation ---")
print(f"Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(report)

#vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
#X = vectorizer.fit_transform(df['processed_text'])
#y = df['sentiment']

#print(df[5:])
#print(classification_report(y_test, y_pred))
#print(accuracy_score(y_test, y_pred))

"""# Social Media Sentiment Analysis Model

This project uses Natural Language Processing (NLP) to classify the sentiment of social media posts.

## Key Features
- Processes and cleans raw text data using NLTK.
- Implements advanced lemmatization with Part-of-Speech (POS) tagging.
- Uses TF-IDF with N-grams for feature engineering.
- Trains a Multinomial Naive Bayes classifier to predict sentiment (Positive, Negative, Neutral).

## Technologies Used
- Python
- Pandas
- NLTK
- Scikit-learn
- Google Colab

# Social Media Sentiment Analysis Model

This project uses Natural Language Processing (NLP) to classify the sentiment of social media posts.

## Key Features
- Processes and cleans raw text data using NLTK.
- Implements advanced lemmatization with Part-of-Speech (POS) tagging.
- Uses TF-IDF with N-grams for feature engineering.
- Trains a Multinomial Naive Bayes classifier to predict sentiment (Positive, Negative, Neutral).

## Technologies Used
- Python
- Pandas
- NLTK
- Scikit-learn
- Google Colab
"""



import kagglehub

# Download latest version
path = kagglehub.dataset_download("jp797498e/twitter-entity-sentiment-analysis")

print("Path to dataset files:", path)

import os
print(os.listdir('/kaggle/input/twitter-entity-sentiment-analysis'))

#Function to remove words like is, the etc from the tweets
def remove_stopwords(text):
  tokens = text.split()
  return ' '.join([word for word in tokens if word not in stop_words])

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet,stopwords
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
# finds POS tags of words from text and Normalize them to Lemma Tags(NOUN,VERB, ADJECTIVE etc...)
def nltk_tag_to_wordnet_tag(nltk_tag):
  if nltk_tag.startswith('J'):
    return wordnet.ADJ
  elif nltk_tag.startswith('V'):
    return wordnet.VERB
  elif nltk_tag.startswith('N'):
    return wordnet.NOUN
  elif nltk_tag.startswith('R'):
    return wordnet.ADV
  else:
    return None
#Lemmatization Function : Maps the normalized lemma tags to each word in the text
def lemmatize_text(text):
  lemmatizer = WordNetLemmatizer()
  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))
  wordnet_tagged = map(lambda x: (x[0],nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)
  lemmatized_sentence = [lemmatizer.lemmatize(word, tag) if tag else word for word, tag in wordnet_tagged]
  return ' '.join(lemmatized_sentence)

#Function to clean data by removing links,hastags and mentions from the tweets

def clean_text(text):
  text = re.sub(r'http\S+|www\S+|https\S+', '',text, flags = re.MULTILINE)
  text = re.sub(r'@\w+', '', text)
  text = re.sub (r'[^A-Za-z\s]', '',text)
  return text.strip().lower()